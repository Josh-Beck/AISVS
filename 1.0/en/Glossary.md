
## Glossary of AI Security Terms

> *This comprehensive glossary provides definitions of key AI, ML, and security terms used throughout the AISVS to ensure clarity and common understanding.*

**Adversarial Example**: An input deliberately crafted to cause an AI model to make a mistake, often by adding subtle perturbations imperceptible to humans.

**Agentic AI**: AI systems that can operate with some degree of autonomy to achieve goals, often making decisions and taking actions without direct human intervention.

**Data Poisoning**: The deliberate corruption of training data to compromise model integrity, often to install backdoors or degrade performance.

**Differential Privacy**: A mathematical framework that provides formal guarantees about privacy protection when analyzing or sharing data.

**Embeddings**: Dense vector representations of data (text, images, etc.) that capture semantic meaning in a high-dimensional space.

**Federated Learning**: A machine learning approach where models are trained across multiple decentralized devices holding local data samples, without exchanging the data itself.

**Hallucination**: When an AI model generates content that is factually incorrect or fabricated while presenting it as truthful.

**Jailbreak**: Techniques used to circumvent safety guardrails in AI systems, particularly in large language models, to produce prohibited content.

**LIME (Local Interpretable Model-agnostic Explanations)**: A technique to explain the predictions of any machine learning classifier by approximating it locally with an interpretable model.

**Membership Inference Attack**: An attack that aims to determine whether a specific data point was used to train a machine learning model.

**Model Extraction**: An attack where an adversary repeatedly queries a target model to create a functionally similar copy without authorization.

**Model Inversion**: An attack that attempts to reconstruct training data by analyzing model outputs.

**Multi-agent System**: A system composed of multiple interacting AI agents, each with potentially different capabilities and goals.

**Prompt Injection**: An attack where malicious instructions are embedded in inputs to override a model's intended behavior.

**RAG (Retrieval-Augmented Generation)**: A technique that enhances large language models by retrieving relevant information from external knowledge sources before generating a response.

**SBOM (Software Bill of Materials)**: A formal record containing the details and supply chain relationships of various components used in building software or AI models.

**SHAP (SHapley Additive exPlanations)**: A game theoretic approach to explain the output of any machine learning model by computing the contribution of each feature to the prediction.

**Vector Database**: A specialized database designed to store high-dimensional vectors (embeddings) and perform efficient similarity searches.